{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 - Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References and Resources*\n",
    "- Article Discussing Classification - https://emeritus.org/blog/artificial-intelligence-and-machine-learning-classification-in-machine-learning/ \n",
    "- Article on Logistic Regression - https://www.ibm.com/topics/logistic-regression\n",
    "- Article on Metrics - https://www.purestorage.com/knowledge/machine-learning-performance-metrics.html#:~:text=Machine%20learning%20performance%20metrics%20help%20evaluate%20and%20compare%20different%20machine,and%20ROC%20curve%2C%20among%20others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classficiation\n",
    "\n",
    "In machine learning, classification is a predictive modelling problem where the class label is anticiaptaed for a specific example of input data. \n",
    "\n",
    "Features of Classification:\n",
    "- It help categorise data into different classes.\n",
    "- It requires the help of labelled input and output data from human supervision to to predict accurate outcomes for new and unseen data. \n",
    "- The pre-labelled data is used to train the algorithm to categorise fiel types, such as images, words and documents, and then to predict the outcomes by learning the new data via learning patterns.\n",
    "- Classification can be performed on structured and unstructured data to accurately predict whether or not the data will fall into predtermined categories.\n",
    "\n",
    "Unlike regression, which is used to predict the output for real or continuous values of a predictive relationship between dependent and independent variables; classification is used to identify specific categories of new observations based on one or more independent variable. \n",
    "\n",
    "Accordingly. classification has applications in the folowing;\n",
    "- Image Classification\n",
    "- Fraud Detection\n",
    "- Spam Filtering\n",
    "- Facial Recognition\n",
    "- Product Categorization\n",
    "\n",
    "#### Binary Classification: \n",
    "Binary is a type of problem in classification in machine learning that has only two possible outcomes. For example, yes or no, true or false, spam or not spam, etc. Some common binary classification algorithms are logistic regression, decision trees, simple bayes, and support vector machines. \n",
    "\n",
    "#### Multi-Class Classification:\n",
    "Multi-class is a type of classification problem with more than two outcomes and does not have the concept of normal and abnormal outcomes. Here each outcome is assigned to only one label. For example, classifying images, classifying species, and categorizing faces, among others. Some common multi-class algorithms are choice trees, progressive boosting, nearest k neighbors, and rough forest.\n",
    "\n",
    "#### Multi-Label Classification:\n",
    "Multi-label is a type of classification problem that may have more than one class label assigned to the data. Here the model will have multiple outcomes. For example, a book or a movie can be categorized into multiple genres, or an image can have multiple objects. Some common multi-label algorithms are multi-label decision trees, multi-label gradient boosting, and multi-label random forests. \n",
    "\n",
    "An accuracy score is a measure how probably the classification algorithm is going to be accurate.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "\n",
    "Note confusing name as we are not trying to predict a continous value. \n",
    "\n",
    "Logistic regression is a data analysis technique that uses mathematics to find the relationships between two data factors. It then uses this relationship to predict the value of one of those factors based on the other. The prediction usually has a finite number of outcomes, like yes or no.\n",
    "\n",
    "Since the outcome is a probability, the dependent variable is bounded between 0 and 1. In logistic regression, a logit transformation is applied on the odds—that is, the probability of success divided by the probability of failure. This is also commonly known as the log odds, or the natural logarithm of odds, and this logistic function is represented by the following formulas: \n",
    "\n",
    "- Logit(pi) = 1/(1+ exp(-pi))\n",
    "\n",
    "After the model has been computed, it’s best practice to evaluate the how well the model predicts the dependent variable, which is called goodness of fit.The Hosmer–Lemeshow test is a popular method to assess model fit.\n",
    "\n",
    "We do predict a continous values (between 0 and 1) but we then threshold it, to take on discrete (non-continuous) values. So for example, if we add a threshold of 0.5 we can say that any value that is above 0.5 is true and any value that is below 0.5 is false, in this way the 1 or 0 results become true or false. The repose variable y is either 0 or 1, and the *predicted* value is between 0 and 1. \n",
    "\n",
    "The sigmoid function takes any real valued number and maps it to a value between 0 and 1. Gives an s shaped curve on graph. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Metrics\n",
    "\n",
    "Machine learning performance metrics help evaluate and compare different machine learning models by providing quantitative measures of a model's accuracy, precision, recall, F1 score and ROC curve. Metrics help us understand what’s working, what’s not, and why. Just like anything else, we can measure the performance of machine learning to gauge the effectiveness of our machine learning models. \n",
    "\n",
    "The procedure of systematically choosing a set of predictors (parameters) that have a significant relationship with the response variable is called variable selection.\n",
    "\n",
    "**Types of Machine Learning Perofmace Metrics**\n",
    "\n",
    "- **Accuracy** is the most straightforward metric. It’s the ratio of correctly predicted instances to total instances in the data set. Accuracy is useful for balanced data sets when all classes are equally important.\n",
    "\n",
    "- **Precison** focuses on the fraction of relevant instances among the retrieved instances. It’s the ability of the classifier not to label a sample that is negative as positive. Precision is crucial when the cost of false positives is high, such as in medical diagnosis or fraud detection.\n",
    "    - Precision is defined as the number of relevant retrieved instances, divided by the total number of retrieved instances in the category.\n",
    "    - *Precision* = Total Postives/ Total Positives + False Positives \n",
    "\n",
    "- **Recall (Sensitivity)** measures the ability of the classifier to find all the relevant cases within a data set. It’s the ability of the classifier to find all the positive samples. Recall is important when missing positive instances (false negatives) is more critical than having false positives. For example, in cancer detection, it's crucial to catch all actual cases even if it means some false alarms.\n",
    "\n",
    "    - Recall is defined as the number of relevant retrieved instances, divided by the total number of relevant instances:\n",
    "    - *Recall* = Total Positives/ Total Positives + Total Negatives\n",
    "\n",
    "- **F1 Score** is the harmonic mean of precision and recall, providing a balanced measure that considers both false positives and false negatives. It's especially useful when dealing with imbalanced data sets. Use the F1 score when you want to balance precision and recall and there is an uneven class distribution or when false positives and false negatives carry similar weights.\n",
    "\n",
    "- **ROC Curve and AUC** The receiver operating characteristic (ROC) curve plots the true positive rate (recall) against the false positive rate for different thresholds. The area under the ROC curve (AUC) provides an aggregate measure of performance across all thresholds. ROC curves and AUC are particularly useful in binary classification tasks to understand the trade-offs between true positives and false positives at different decision thresholds. AUC is useful for imbalance and threshold selection.\n",
    "\n",
    "- **Specificity** Specificity measures the proportion of actual negative cases that are correctly identified as negative by the classifier. It complements recall (sensitivity) by focusing on true negatives. Specificity is important in scenarios where correctly identifying negative cases is crucial, such as in disease screening tests where false alarms can lead to unnecessary treatments or costs.\n",
    "\n",
    "- **Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)** These metrics are commonly used in regression tasks to measure the average magnitude of errors between predicted and actual values. MAE and RMSE are suitable for regression problems where the absolute magnitude of errors is important, such as predicting housing prices or sales forecasts.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
